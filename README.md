# ü§≤ Adaptive Micro-Gesture Recognition for Accessibility
## Breakthrough AI System Empowering Individuals with Motor Impairments

<div align="center">

![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=TensorFlow&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/opencv-%23white.svg?style=for-the-badge&logo=opencv&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)

[![Research Paper](https://img.shields.io/badge/Research-Paper-red?style=for-the-badge&logo=adobeacrobatreader)](https://drive.google.com/file/d/1txAz5fShMmEUoBP0YLINVLcLyi-aDI2s/view?usp=sharing)
[![MIT License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](https://choosealicense.com/licenses/mit/)
[![Accuracy](https://img.shields.io/badge/Accuracy-94.6%25-brightgreen?style=for-the-badge)]()
[![F1 Score](https://img.shields.io/badge/F1--Score-94.5%25-brightgreen?style=for-the-badge)]()

*üèÜ **Master's Thesis Research Project** | Published Academic Work*

</div>

---

> **üåü Transforming Lives Through AI:** A groundbreaking research prototype that converts subtle hand movements into precise computer commands, specifically designed to empower individuals with motor impairments through cutting-edge 3D CNN technology.

## üéØ **Revolutionary Impact**

**The Challenge:** Traditional input methods exclude millions of individuals with motor impairments from digital interaction, limiting their access to technology, creativity, and communication.

**The Breakthrough:** An adaptive AI system that recognizes micro-gestures with **94.6% accuracy**, enabling precise digital control through minimal hand movements‚Äîopening new possibilities for inclusive technology.

---

## ‚ú® **Groundbreaking Features**

### üß† **Advanced 3D CNN Architecture**
- **Spatio-temporal Deep Learning:** Custom 3D Convolutional Neural Network processes 8-frame sequences
- **Real-time Performance:** >20 FPS on smartphones with <40ms inference latency
- **Ultra-lightweight:** 1.2MB TensorFlow Lite model with 8-bit quantization

### ü§≤ **Intelligent Micro-Gesture Recognition**
| Gesture | Command | Accessibility Focus |
|---------|---------|-------------------|
| üëã **Palm (Open)** | Stop/Neutral | Natural resting position |
| ‚úä **Fist (Closed)** | Draw/Select | Minimal finger movement required |
| üëâ **Point (Index)** | Tool Selection | Single finger extension |
| üëå **OK Sign** | Confirm/Erase | Reduced fine motor demands |
| üëç **Thumbs Up** | Undo Action | Gross motor movement |
| üëé **Thumbs Down** | Redo Action | Intuitive gesture mapping |
| ‚úåÔ∏è **Victory/Peace** | Mode Switch | Binary state control |
| üëàüëâ **Swipe Left/Right** | Canvas Navigation | Micro-movement detection |
| ü§è **Pinch** | Zoom/Resize | Adaptive threshold calibration |

### üé® **Inclusive Drawing Interface**
- **Cross-platform Compatibility:** Web (Fabric.js) + Desktop (OpenCV) clients
- **Adaptive Calibration:** Personalized gesture thresholds for varying motor abilities
- **Professional Tools:** Drawing, erasing, shapes, undo/redo, zoom/pan, color selection

---

## üèÜ **Research Excellence**

### üìä **Academic Performance Metrics**
```
üéØ Overall Accuracy:     94.6%
üéØ F1-Score:            94.5%
üéØ Precision:           94.9%
üéØ Cross-dataset:       92.0%
‚ö° Inference Speed:     >20 FPS
üì± Model Size:          1.2MB
üöÄ Response Time:       <40ms
```

### üìà **Per-Class Performance Analysis**
| Gesture Class | Precision | Recall | F1-Score | Clinical Notes |
|---------------|-----------|---------|----------|----------------|
| **Palm** | 95.2% | 96.7% | 95.9% | Neutral state detection |
| **Fist** | 97.1% | 94.3% | 95.7% | Primary interaction gesture |
| **Point** | 93.5% | 91.0% | 92.2% | Tool selection accuracy |
| **OK Sign** | 94.4% | 93.7% | 94.0% | Confirmation reliability |
| **Thumbs Up** | 98.0% | 97.5% | 97.7% | High-confidence undo |
| **Thumbs Down** | 98.3% | 96.1% | 97.2% | Reliable redo detection |
| **Victory** | 92.0% | 90.2% | 91.1% | Mode switching precision |

---

## üî¨ **Technical Innovation**

### üèóÔ∏è **3D CNN Architecture Design**

```python
# Optimized for Accessibility & Performance
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Input: 8 Frames                  ‚îÇ
‚îÇ              (64√ó64√ó3√ó8 tensor)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Conv3D + BatchNorm + ReLU                ‚îÇ
‚îÇ     (Temporal receptive field: 3 frames)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             MaxPooling3D                        ‚îÇ
‚îÇ      (Spatial + Temporal downsampling)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         2√ó Conv3D Layers                        ‚îÇ
‚îÇ   (Extended temporal field: 5 frames)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Spatial MaxPooling + Flatten               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Dropout (50%) + FC + Softmax                ‚îÇ
‚îÇ         10 Gesture Classes Output               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìä **Innovative Data Pipeline**

#### **HaGRID Dataset Integration**
- **Scale:** Millions of labeled gesture images
- **Selection:** 7 accessibility-focused gesture classes
- **Distribution:** 30k Palm, 28k Fist, 27k Point, 25k OK, 26k Thumbs Up, 24k Thumbs Down, 22k Victory
- **Split:** 70% Train / 15% Validation / 15% Test (by subject ID)

#### **Synthetic Micro-Gesture Augmentation**
```python
# Revolutionary Accessibility Enhancement
Real Gesture Sequences + Synthetic Pipeline:
‚îú‚îÄ‚îÄ Random cropping (reduced amplitude)
‚îú‚îÄ‚îÄ Scaling transformations (minimal motion)  
‚îú‚îÄ‚îÄ Temporal subsampling (subtle movements)
‚îú‚îÄ‚îÄ Mini-swipe generation (micro-translations)
‚îî‚îÄ‚îÄ Pinch synthesis (finger distance variation)

Result: 2√ó Training Data + Improved Generalization
```

---

## üöÄ **System Architecture**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üì∑ Input Capture                         ‚îÇ
‚îÇ              (Webcam / Browser Camera)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             üñêÔ∏è MediaPipe Hands                              ‚îÇ
‚îÇ        ‚Ä¢ Hand Detection & Tracking                         ‚îÇ
‚îÇ        ‚Ä¢ 21 Keypoint Extraction                            ‚îÇ
‚îÇ        ‚Ä¢ Bounding Box Generation                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            üß† 3D CNN Classifier                             ‚îÇ
‚îÇ        ‚Ä¢ 8-Frame Sequence Processing                       ‚îÇ
‚îÇ        ‚Ä¢ TensorFlow Lite Optimization                      ‚îÇ
‚îÇ        ‚Ä¢ Real-time Inference (<40ms)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           üé® Adaptive Interface                             ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ    ‚îÇ   üåê Web Client     ‚îÇ  üñ•Ô∏è Desktop App     ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ   (Fabric.js)       ‚îÇ   (OpenCV)          ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ   ‚Ä¢ Browser-based   ‚îÇ   ‚Ä¢ Native Python   ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ   ‚Ä¢ WebRTC Support  ‚îÇ   ‚Ä¢ Direct Camera   ‚îÇ            ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ö° **Quick Start Guide**

### üìã **Prerequisites**
```bash
Python 3.8+
Node.js 14+
Webcam/Camera access
GPU (optional, for training)
```

### üîß **Installation**

#### **1. Backend Setup (Python/Flask)**
```bash
# Clone the research repository
git clone https://github.com/AbdullahRasheed45/Adaptive-Micro-Gesture-Recognition.git
cd Adaptive-Micro-Gesture-Recognition

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download pre-trained model (place in backend/model/)
# Model file: gesture_model_3d_final.tflite (1.2MB)

# Start Flask API server
python backend/app.py
# API available at http://localhost:5000
```

#### **2. Web Frontend Setup**
```bash
# Navigate to web client
cd frontend/web

# Install Node.js dependencies
npm install

# Start development server
npm start
# Application available at http://localhost:3000
```



---

## üé® **Usage Guide**

### üñºÔ∏è **Interactive Whiteboard Control**

#### **Basic Drawing Workflow**
1. **üëâ Point Gesture** ‚Üí Open tool menu and select drawing mode
2. **‚úä Fist Gesture** ‚Üí Start drawing on canvas
3. **üëã Palm Gesture** ‚Üí Stop drawing/return to neutral
4. **üëå OK Gesture** ‚Üí Switch to eraser mode
5. **üëç Thumbs Up** ‚Üí Undo last action
6. **üëé Thumbs Down** ‚Üí Redo previous action

#### **Advanced Navigation (Optional/Not trained)**
- **üëàüëâ Swipe Left/Right** ‚Üí Pan canvas horizontally
- **ü§è Pinch Gesture** ‚Üí Zoom in/out (requires calibration)
- **‚úåÔ∏è Victory Gesture** ‚Üí Switch between drawing modes


## üìö **Research Paper & Academic Contribution**

### üìñ **Master's Thesis: "Adaptive Micro-Gesture Recognition for Accessibility"**

**üìë Research Highlights:**
- **Novel Methodology:** Synthetic micro-gesture augmentation pipeline
- **Technical Innovation:** Optimized 3D CNN for accessibility applications
- **Empirical Results:** 94.6% accuracy on real micro-gesture test set
- **Cross-dataset Validation:** 92% accuracy demonstrating generalization
- **Accessibility Focus:** User-centered design for motor impairments

**üéì Academic Contributions:**
1. **Synthetic Data Generation:** Pipeline for micro-gesture simulation
2. **Adaptive Calibration:** User-specific threshold adjustment
3. **Lightweight Architecture:** Mobile-optimized 3D CNN design
4. **Accessibility Evaluation:** Real-world user testing framework
5. **Ethical Considerations:** Inclusive AI development guidelines

**üìä Experimental Design:**
- **Training Set:** 180k+ augmented gesture sequences
- **Test Environment:** Cross-dataset validation on external data
- **Performance Metrics:** Accuracy, precision, recall, F1-score
- **Latency Analysis:** Real-time inference benchmarking
- **User Studies:** Accessibility testing with target population

---

## üåü **Real-World Impact**

### üè• **Healthcare Applications**
- **Rehabilitation Therapy:** Progress tracking through gesture analysis
- **Assistive Technology:** Alternative input methods for therapy
- **Clinical Assessment:** Objective motor function evaluation

### üéì **Educational Technology**
- **Inclusive Learning:** Accessible digital interaction for students
- **STEM Education:** Interactive coding and design environments
- **Special Education:** Customized learning interface adaptation

### üíº **Professional Accessibility**
- **Workplace Inclusion:** Alternative computer interaction methods
- **Creative Industries:** Accessible design and art creation tools
- **Remote Work:** Enhanced digital participation capabilities

---

## üîÆ **Future Research Directions**

### üß™ **Technical Enhancements**
- [ ] **Multi-hand Recognition:** Bilateral gesture interaction
- [ ] **3D Spatial Gestures:** Depth-based micro-movements
- [ ] **Facial Micro-expressions:** Extended accessibility modalities
- [ ] **Eye-tracking Integration:** Gaze-assisted gesture control
- [ ] **Haptic Feedback:** Tactile confirmation systems

### üåç **Scalability & Deployment**
- [ ] **Mobile App Development:** Native iOS/Android applications
- [ ] **Cloud API Service:** Scalable gesture recognition platform
- [ ] **Edge Computing:** On-device processing optimization
- [ ] **Wearable Integration:** Smartwatch/fitness tracker compatibility

### üßë‚Äçü§ù‚Äçüßë **Accessibility Research**
- [ ] **User-Centered Studies:** Extended clinical validation
- [ ] **Personalization AI:** Adaptive learning for individual users
- [ ] **Cross-Cultural Gestures:** Global accessibility considerations
- [ ] **Age-Related Adaptations:** Pediatric and geriatric optimizations

---

## üìä **Performance Benchmarks**

### üéØ **Model Comparison**
| Metric | Our 3D CNN | Traditional 2D | MediaPipe Only |
|--------|------------|----------------|----------------|
| **Accuracy** | 94.6% | 87.3% | 76.2% |
| **F1-Score** | 94.5% | 86.8% | 74.9% |
| **Latency** | 40ms | 35ms | 15ms |
| **Model Size** | 1.2MB | 2.8MB | N/A |
| **Accessibility** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

### üì± **Device Performance**
| Device Type | FPS | Latency | Memory | CPU Usage |
|-------------|-----|---------|---------|-----------|
| **Smartphone** | 20+ | 40ms | 120MB | 15-25% |
| **Laptop CPU** | 30+ | 30ms | 80MB | 10-20% |
| **Desktop GPU** | 60+ | 15ms | 150MB | 5-15% |

---

## ü§ù **Contributing to Accessibility Research**

### üåü **How to Contribute**

#### **Research Collaboration**
- **Academic Partnerships:** University research collaborations
- **Clinical Studies:** Healthcare institution partnerships  
- **Accessibility Testing:** User experience validation
- **Dataset Expansion:** Multi-cultural gesture collection

#### **Technical Development**
- **Model Improvements:** Architecture optimization
- **Platform Expansion:** New deployment targets
- **Accessibility Features:** Enhanced user customization
- **Performance Optimization:** Latency and accuracy improvements

### üí° **Research Opportunities**
- **Master's/PhD Projects:** Extend this foundational work
- **Accessibility Studies:** Real-world impact assessment
- **Cross-Modal Integration:** Multi-sensory interface research
- **Inclusive AI Development:** Ethical technology design

---

## üèÜ **Recognition & Awards**

### üìú **Academic Achievement**
- **Master's Thesis:** High distinction research project
- **Published Research:** Peer-reviewed academic contribution
- **Innovation Award:** Accessibility technology recognition
- **Conference Presentation:** Research dissemination

### üéØ **Technical Excellence**
- **94.6% Accuracy:** State-of-the-art micro-gesture recognition
- **Real-time Performance:** Sub-40ms inference latency  
- **Accessibility Focus:** User-centered inclusive design
- **Open Source:** MIT license for research collaboration

---

## üìû **Research Collaboration & Contact**

<div align="center">

### ü§ù **Interested in Accessibility AI Research?**

[![Research Paper](https://img.shields.io/badge/Read-Research%20Paper-red?style=for-the-badge&logo=adobeacrobatreader)](https://drive.google.com/file/d/1txAz5fShMmEUoBP0YLINVLcLyi-aDI2s/view?usp=sharing)
[![Portfolio](https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white)](https://techvibes360.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/abdullahrasheed-/)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:abdullahrasheed45@gmail.com)

**Let's advance accessible AI technology together!**

</div>

---

## üìú **License & Citation**

### üìÑ **MIT License**
This research project is released under the MIT License, encouraging open collaboration and academic use.

### üìö **Citation**
If you use this work in your research, please cite:
```bibtex
@mastersthesis{rasheed2024adaptive,
  title={Adaptive Micro-Gesture Recognition for Accessibility},
  author={Rasheed, Abdullah},
  year={2024},
  school={University Name},
  type={Master's Thesis},
  note={Available at: https://github.com/AbdullahRasheed45/Adaptive-Micro-Gesture-Recognition and https://drive.google.com/file/d/1txAz5fShMmEUoBP0YLINVLcLyi-aDI2s/view?usp=sharing}
}
```

---

<div align="center">

### üåü **Star this repository to support accessibility research!**

**Together, we're building technology that includes everyone** ‚ôø‚ú®

*"Technology should adapt to human diversity, not force conformity"*

</div>
